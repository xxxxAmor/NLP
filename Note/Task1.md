##                                       赛题理解 

#### 赛题名称

零基础入门NLP之新闻文本分类挑战赛。

赛题以自然语言处理为背景，要求选手根据新闻文本字符对新闻的类别进行分类，这是一个经典文本分类问题。通过这道赛题可以引导大家走入自然语言处理的世界，带大家接触NLP的预处理、模型构建和模型训练等知识点。



#### 赛题数据

赛题数据为新闻文本，并按照字符级别进行匿名处理。整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐

赛题数据由以下几个部分构成：[训练集20w条样本](https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/train_set.csv.zip)，[测试集A包括5w条样本](https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/test_a.csv.zip)，[测试集B包括5w条样本](https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/test_a.csv.zip)。为了预防选手人工标注测试集的情况，我们将比赛数据的文本按照字符级别进行了匿名处理。处理后的赛题训练数据如下：



#### 数据标签



![Uh64y9.png](https://s1.ax1x.com/2020/07/20/Uh64y9.png)



在数据集中标签的对应的关系如下：

{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}



#### 评测标准

评价标准为类别f1_score的均值，提交结果与实际测试集的类别进行对比，结果越大越好。



#### 解题思路

文本分类流程：

[![UTPgaD.png](https://s1.ax1x.com/2020/07/21/UTPgaD.png)](https://imgchr.com/i/UTPgaD)

`特征提取`：一般来说，文本和文档都是非结构化数据集，然而，当数学建模成为分类器的一部分时，这些非结构化文本序列必须转化到结构化的特征空间。常用的特征提取技术有词频逆文档频率（TF-idf), 词频，word2vec等

文本特征选择是从原始特征中基于一些准则来选择那些最能将类别区分开的特征词。特征选择筛选出相关性较低和多余的特征并将它们删去，使处理效率大大提升。文本数据的特征选择研究的重点就是用来衡量单词重要性的评估函数，其过程就是首先根据这个评估函数来给每一个单词计算出一个重要性的值，然后根据预先设定好的阈值来选择出所有其值超过这个阈值的单词。目前，国内外常用文本特征选择方法主要有以下几种： 文档频率、信息增益、互信息、统计量、期望交叉熵等。

-  信息增益

在信息增益中，以特征能给分类程序带来的信息量来作为度量标准的，带来的信息与特征的重要程度成正比。是否有这个特征将为系统的信息量带来波动，而信息量的差值即为系统中受它影响的信息量，换言之即为熵。设有变量X，有n种取值可能，Pi为每种可能被取值的概率，则定义X的熵
$$
H (  X )=  ∑
p(x i )logp(x i )
$$
换言之， X变化的可能性与其能带来的信息量成正比， 也就是与熵成正比。 对聚类来说，就是文档属于哪个类别的变化越多， 类别的信息量就越大。 所以特征 T给聚类C 或分类 C 带来的信息增益为 IG ( T) = H (  C） - H ( C|T ）。 H (  C|T） 有两种可能：一种是出现特征， 将其记为 T， 用 t表示， 一种是特征 T不出现，用 t' 表示。 所以 H (  C|T） = P ( T） H ( C|t ）+ P ( t' ）H ( C|t' ）， 再通过熵的计算公式求出特征与类别的信息增益公式。

信息增益的最大的不便就是只能判断特征在整个系统中的影响，而判断它在哪个类别中。



-  文档频率

在文档频率方法中，使用特征词在一个类别中出现的文档数来表示这个特征词与该类别的相关度。更大概率通过筛选的特征词是在更多的文档中出现过的。

文档频率是最简易的特征抽取方法，由于它有基于训练语料规模的线性计算复杂度，更适合大规模的语料统计，能够极大地使效果改善。



-  互信息

互信息（Mutual Information）是基于信息熵概念上的，它是度量两个随机事件相关性的特征，广泛使用于统计语言模型中。词条（记为t）和文本类别（记为c）的互信息定义是：
$$
MI (  t,c) = log
 \frac {P  ( tc)} {P ( t) × P (c )}
$$
其中 P ( tc) ∈C 且包含 t的文档概率， P (  t） 表示包含词条t的语料中的文本的概率， P (  c ）表示C类文本在语料中出现的概率。 根据概率，如果在分布上一个词与一个类别是在统计上独立的， 即 P (  tc） = P (  t） × P (  c） ,则 MI ( t,c） =0， 也就是说词 t的频率无法对预测类别C产生影响。在实际运用中，互信息表达式可近似为语料库中对应的出现频数。 如果包含 t且属于 C 的文档频数(记为 X)， 包含 t但不属于C 的文档频数(记为Y)，属于C 但不包含t的文档频数(记为Z)，语料中文本总数(记为N)，则有：
$$
MI (  t,C） = log
\frac{X× N}
{(  X+ Y )× (X+Z )}
$$
对于属于不止一个类别的应用， 算出 t在每一类中的 MI值，再算出对整个语料而言t的MI值：
$$
MI ( t) = max
mMI (  t,C_i )
$$
互信息计算的时间复杂度与信息增益相似。互信息的缺点是评价结果受到词条频率影响较大，且之前的计算量很大。



-  期望交叉熵

如果词条t和类别Ci相关性越强，则越大，如果与此同时，又很小，则说明该词对该类的影响大。这个量代表了文本类别的概率分布，以及文本类别在某种特征的基础上的概率分布之间的距离。



![UTiVzR.png](https://s1.ax1x.com/2020/07/21/UTiVzR.png)

`分类方法`：

文本分类是依照文本内容或特征，在规定的分类系统下将待划分文本分配到一个及以上的之前定义好的分类中的方法  。文本分类是一一对应的方法，将未明确的待分类文本对应到已定义的分类中， 由于一篇文本可以同多个类别相关联， 这个映射一般来说是一对一或一对多的映射。 数学公式为：
f：X→Y 其中：X=(M 1 ,M 2 ,…,M n ) Y = ( N 1 ,N 2 ,…,N m ) 即： X为所有待划分的文本的集合； Y 为规定的分类系统下， 所有分类的集合。 X 可以为无限集合， 而 Y 必须为有限集合。

分类方式一般依照基本划分方法不同而分为两种：基于机器学习的分类方法和基于规则的分类方法。